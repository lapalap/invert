{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kirillbykov/Documents/GitHub/invert/invert/explainer.py:61: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  self.labels = torch.BoolTensor(torch.BoolStorage.from_file(labels_path,\n",
      "/Users/kirillbykov/opt/anaconda3/lib/python3.9/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "/Users/kirillbykov/opt/anaconda3/lib/python3.9/site-packages/torch/storage.py:899: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = cls(wrap_storage=untyped_storage)\n"
     ]
    }
   ],
   "source": [
    "import sympy, torch, sympytorch, pandas\n",
    "from invert.explainer import Invert\n",
    "from invert.metrics import Metric\n",
    "\n",
    "explainer = Invert(\n",
    "        storage_dir = \".invert/\"\n",
    "    )\n",
    "\n",
    "explainer.load_concept_labels(labels_path='/Users/kirillbykov/Documents/GitHub/invert/assets/imagenet/one_hot_labels.tnsr',\n",
    "                              num_samples=50000,\n",
    "                              num_concepts=1473,\n",
    "                              description_path='/Users/kirillbykov/Documents/GitHub/invert/assets/imagenet/ILSVRC2012_classes.json'\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.load('/Users/kirillbykov/Documents/Work/DORA/notebooks/feature_extractor/features/kaggle/resnet18/resnet18_vals_full/A50k_avgpoolresnet18_val.tnsr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = A[:, 296]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kirillbykov/opt/anaconda3/lib/python3.9/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/Users/kirillbykov/opt/anaconda3/lib/python3.9/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: No negative samples in targets, false positive value should be meaningless. Returning zero tensor in false positive score\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+\n",
      "+\n",
      "New length\n",
      "+\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "logical_and() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/kirillbykov/Documents/GitHub/invert/test.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/kirillbykov/Documents/GitHub/invert/test.ipynb#X20sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m explainer\u001b[39m.\u001b[39;49mexplain_representation_test(A, L \u001b[39m=\u001b[39;49m \u001b[39m3\u001b[39;49m, B \u001b[39m=\u001b[39;49m \u001b[39m3\u001b[39;49m, metric\u001b[39m=\u001b[39;49mMetric(), min_fraction\u001b[39m=\u001b[39;49m\u001b[39m0.002\u001b[39;49m, max_fraction\u001b[39m=\u001b[39;49m\u001b[39m0.5\u001b[39;49m)\n",
      "File \u001b[0;32m~/Documents/GitHub/invert/invert/explainer.py:130\u001b[0m, in \u001b[0;36mInvert.explain_representation_test\u001b[0;34m(self, A, L, B, metric, batch_size, limit_search, min_fraction, max_fraction, mode, memorize_states)\u001b[0m\n\u001b[1;32m    128\u001b[0m conjunction \u001b[39m=\u001b[39m BEAM[i][\u001b[39m\"\u001b[39m\u001b[39mformula\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m&\u001b[39m ATOMIC_CONCEPTS[j][\u001b[39m\"\u001b[39m\u001b[39mformula\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    129\u001b[0m \u001b[39mif\u001b[39;00m conjunction \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 130\u001b[0m     _formula_buffer \u001b[39m=\u001b[39m conjunction(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmemdict)\n\u001b[1;32m    131\u001b[0m     _metric \u001b[39m=\u001b[39m metric(A, _formula_buffer)\n\u001b[1;32m    132\u001b[0m     _concept_fraction \u001b[39m=\u001b[39m _formula_buffer\u001b[39m.\u001b[39msum()\u001b[39m/\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_samples\n",
      "File \u001b[0;32m~/Documents/GitHub/invert/invert/phi.py:30\u001b[0m, in \u001b[0;36mPhi.__call__\u001b[0;34m(self, memdict)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, memdict: \u001b[39mdict\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mtensor:\n\u001b[1;32m     29\u001b[0m     \u001b[39m# for binary labels\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_pytorch_expr(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmemdict)[:, \u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sympytorch/sympy_module.py:264\u001b[0m, in \u001b[0;36mSymPyModule.forward\u001b[0;34m(self, **symbols)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39msymbols: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[0;32m--> 264\u001b[0m     out \u001b[39m=\u001b[39m [node(symbols) \u001b[39mfor\u001b[39;00m node \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_nodes]\n\u001b[1;32m    265\u001b[0m     out \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mbroadcast_tensors(\u001b[39m*\u001b[39mout)\n\u001b[1;32m    266\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mstack(out, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sympytorch/sympy_module.py:264\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39msymbols: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[0;32m--> 264\u001b[0m     out \u001b[39m=\u001b[39m [node(symbols) \u001b[39mfor\u001b[39;00m node \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_nodes]\n\u001b[1;32m    265\u001b[0m     out \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mbroadcast_tensors(\u001b[39m*\u001b[39mout)\n\u001b[1;32m    266\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mstack(out, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sympytorch/sympy_module.py:234\u001b[0m, in \u001b[0;36m_Node.forward\u001b[0;34m(self, memodict)\u001b[0m\n\u001b[1;32m    232\u001b[0m         memodict[arg] \u001b[39m=\u001b[39m arg_\n\u001b[1;32m    233\u001b[0m     args\u001b[39m.\u001b[39mappend(arg_)\n\u001b[0;32m--> 234\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_torch_func(\u001b[39m*\u001b[39;49margs)\n",
      "\u001b[0;31mTypeError\u001b[0m: logical_and() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "explainer.explain_representation_test(A, L = 3, B = 3, metric=Metric(), min_fraction=0.002, max_fraction=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from invert.phi import Phi\n",
    "\n",
    "f1 = explainer.concepts[0]|explainer.concepts[2]\n",
    "f2 = f1 | explainer.concepts[1] & explainer.concepts[4]\n",
    "\n",
    "a = Phi(f2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "And"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.expr.func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(n01440764, n01443537, n01484850)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.expr.args[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0): _Node(\n",
       "    (_args): ModuleList(\n",
       "      (0-3): 4 x _Node()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a._pytorch_expr._nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "n01440764 | n01443537 | n01484850 | n01494475"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "logical_or() takes 2 positional arguments but 4 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/kirillbykov/Documents/GitHub/invert/test.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/kirillbykov/Documents/GitHub/invert/test.ipynb#X45sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m a\u001b[39m.\u001b[39;49m_pytorch_expr(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mexplainer\u001b[39m.\u001b[39;49mmemdict)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sympytorch/sympy_module.py:264\u001b[0m, in \u001b[0;36mSymPyModule.forward\u001b[0;34m(self, **symbols)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39msymbols: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[0;32m--> 264\u001b[0m     out \u001b[39m=\u001b[39m [node(symbols) \u001b[39mfor\u001b[39;00m node \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_nodes]\n\u001b[1;32m    265\u001b[0m     out \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mbroadcast_tensors(\u001b[39m*\u001b[39mout)\n\u001b[1;32m    266\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mstack(out, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sympytorch/sympy_module.py:264\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39msymbols: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[0;32m--> 264\u001b[0m     out \u001b[39m=\u001b[39m [node(symbols) \u001b[39mfor\u001b[39;00m node \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_nodes]\n\u001b[1;32m    265\u001b[0m     out \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mbroadcast_tensors(\u001b[39m*\u001b[39mout)\n\u001b[1;32m    266\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mstack(out, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sympytorch/sympy_module.py:234\u001b[0m, in \u001b[0;36m_Node.forward\u001b[0;34m(self, memodict)\u001b[0m\n\u001b[1;32m    232\u001b[0m         memodict[arg] \u001b[39m=\u001b[39m arg_\n\u001b[1;32m    233\u001b[0m     args\u001b[39m.\u001b[39mappend(arg_)\n\u001b[0;32m--> 234\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_torch_func(\u001b[39m*\u001b[39;49margs)\n",
      "\u001b[0;31mTypeError\u001b[0m: logical_or() takes 2 positional arguments but 4 were given"
     ]
    }
   ],
   "source": [
    "a._pytorch_expr(**explainer.memdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Phi(f2) & Phi(explainer.concepts[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "n01443537 & n01491361 & (n01440764 | n01484850)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "logical_and() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/kirillbykov/Documents/GitHub/invert/test.ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/kirillbykov/Documents/GitHub/invert/test.ipynb#X62sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m a\u001b[39m.\u001b[39;49m_pytorch_expr(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mexplainer\u001b[39m.\u001b[39;49mmemdict)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sympytorch/sympy_module.py:264\u001b[0m, in \u001b[0;36mSymPyModule.forward\u001b[0;34m(self, **symbols)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39msymbols: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[0;32m--> 264\u001b[0m     out \u001b[39m=\u001b[39m [node(symbols) \u001b[39mfor\u001b[39;00m node \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_nodes]\n\u001b[1;32m    265\u001b[0m     out \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mbroadcast_tensors(\u001b[39m*\u001b[39mout)\n\u001b[1;32m    266\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mstack(out, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sympytorch/sympy_module.py:264\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39msymbols: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[0;32m--> 264\u001b[0m     out \u001b[39m=\u001b[39m [node(symbols) \u001b[39mfor\u001b[39;00m node \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_nodes]\n\u001b[1;32m    265\u001b[0m     out \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mbroadcast_tensors(\u001b[39m*\u001b[39mout)\n\u001b[1;32m    266\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mstack(out, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sympytorch/sympy_module.py:234\u001b[0m, in \u001b[0;36m_Node.forward\u001b[0;34m(self, memodict)\u001b[0m\n\u001b[1;32m    232\u001b[0m         memodict[arg] \u001b[39m=\u001b[39m arg_\n\u001b[1;32m    233\u001b[0m     args\u001b[39m.\u001b[39mappend(arg_)\n\u001b[0;32m--> 234\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_torch_func(\u001b[39m*\u001b[39;49margs)\n",
      "\u001b[0;31mTypeError\u001b[0m: logical_and() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "a._pytorch_expr(**explainer.memdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [True]\n",
    "\n",
    "a = torch.tensor(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = torch.tensor([False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SymPyModule(expressions=(n01443537 & n01491361 & (n01440764 | n01484850),))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a._pytorch_expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0): _Node(\n",
       "    (_args): ModuleList(\n",
       "      (0): _Node()\n",
       "      (1): _Node(\n",
       "        (_args): ModuleList(\n",
       "          (0): _Node()\n",
       "          (1): _Node(\n",
       "            (_args): ModuleList(\n",
       "              (0-1): 2 x _Node()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a._pytorch_expr._nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy, torch, sympytorch\n",
    "\n",
    "x, y, z = sympy.symbols('x y z')\n",
    "\n",
    "mod = sympytorch.SymPyModule(expressions=[x | y & z])\n",
    "\n",
    "# x_ = torch.rand(3)\n",
    "# out = mod(x_name=x_)  # out has shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memdict = {x: 1, y: 0, z: 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = mod(x = torch.rand(3) > 0., y = torch.rand(3) > 0., z = torch.rand(3) > 0.)  # out has shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True],\n",
       "        [True],\n",
       "        [True]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod(**{x.name: torch.rand(3) > 0., y.name: torch.rand(3) > 0., z.name: torch.rand(3) > 0.})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
